

rule evaluation_clustering_run:
    input:
        rds = cfg.ROOT / "{scenario}/clustering/{method}/{feature_type}/{tile_size}/{distance}/{ndim}/sobj_SNN.RDS",
        df = cfg.ROOT / "{scenario}/clustering/{method}/{feature_type}/{tile_size}/{distance}/{ndim}/seed{seed}/r{resolution}.tsv"
    output: 
        df = cfg.ROOT / "{scenario}/evaluation/{method}/{feature_type}/{tile_size}/{distance}/{ndim}/seed{seed}/r{resolution}_metrics.tsv",
        rds = cfg.ROOT / "{scenario}/evaluation/{method}/{feature_type}/{tile_size}/{distance}/{ndim}/seed{seed}/r{resolution}_evaluation.RDS"
    message:
        """
        Evaluating clustering results.
        dataset: {wildcards.scenario}
        command: {params.cmd}
        feature engineering method: {wildcards.method}, {wildcards.feature_type}, {wildcards.distance}, {wildcards.tile_size}
        ndim: {wildcards.ndim}
        clustering resolution: {wildcards.resolution}
        """
    params:
        cmd     = f"conda run --no-capture-output -n {cfg.r_env} Rscript",
    benchmark:
        str(cfg.ROOT / "{scenario}/evaluation/{method}/{feature_type}/{tile_size}/{distance}/{ndim}/seed{seed}/r{resolution}_evaluation.benchmark")
    shell:
        """
        {params.cmd} scripts/evaluation/do_evaluation.R -i {input.rds} \
            -o {output.rds} -m {output.df} -c {input.df} -l clustering
        """

rule evaluation_latent_run:
    input:
        rds = cfg.ROOT / "{scenario}/clustering/{method}/{feature_type}/{tile_size}/{distance}/{ndim}/sobj_SNN.RDS"
    output: 
        df = cfg.ROOT / "{scenario}/evaluation/{method}/{feature_type}/{tile_size}/{distance}/{ndim}/latent_space_metrics.tsv",
        rds = cfg.ROOT / "{scenario}/evaluation/{method}/{feature_type}/{tile_size}/{distance}/{ndim}/latent_space_evaluation.RDS"
    message:
        """
        Evaluating latent spaces.
        dataset: {wildcards.scenario}
        command: {params.cmd}
        feature engineering method: {wildcards.method}, {wildcards.feature_type}, {wildcards.distance}, {wildcards.tile_size}
        ndim: {wildcards.ndim}
        """
    params:
        cmd     = f"conda run --no-capture-output -n {cfg.r_env} Rscript",
    benchmark:
        str(cfg.ROOT / "{scenario}/evaluation/{method}/{feature_type}/{tile_size}/{distance}/{ndim}/latent_space_evaluation.benchmark")
    shell:
        """
        {params.cmd} scripts/evaluation/do_evaluation.R -i {input.rds} \
            -o {output.rds} -m {output.df} -l latent
        """


int_func, wildcards = cfg.get_all_wildcards(methods=cfg.get_all_methods())
# rule evaluated:
#     input: expand(rules.evaluation_run.output, int_func, **wildcards)
#     message: "Do evaluation"
# print(cfg.get_all_methods())
# print(wildcards)

rule get_metric_file_table:
    input: 
        file_clustering = expand(rules.evaluation_clustering_run.output.df, int_func, **wildcards),
        file_latent = expand(rules.evaluation_latent_run.output.df, int_func, **wildcards),
        root = cfg.ROOT
    output: 
        cfg.ROOT / "metric_file_seed.tsv"
    run:
        import pandas as pd 
        import numpy as np
        file_list1 = input.file_clustering
        file_list2 = [i.removeprefix(input.root+"/") for i in file_list1]
        df1 = pd.DataFrame(file_list2)
        df1 = df1.rename(columns={0: "file"})

        file_list3 = input.file_latent
        file_list4 = [i.removeprefix(input.root+"/") for i in file_list3]
        df2 = pd.DataFrame(file_list4)
        df2 = df2.rename(columns={0: "file"})


        df1[['scenario', 'workflow','method','feature_type','tile_size','distance','ndim','seed','filename']] = df1['file'].str.split("/",  expand=True)
        df1["seed"] = df1["seed"].str.removeprefix("seed")
        df2[['scenario', 'workflow','method','feature_type','tile_size','distance','ndim','filename']] = df2['file'].str.split("/",  expand=True)
        df2["seed"] = np.nan
        df = pd.concat([df1,df2], ignore_index=True)
        df["resolution"] = df["filename"].str.removeprefix("r").str.removesuffix("_metrics.tsv")
        df.to_csv(output[0], sep='\t', header=True, index=False)

# rule get_metric_file_table:
#     input: 
#         file_clustering = expand(rules.evaluation_clustering_run.output.df, int_func, **wildcards),
#         root = cfg.ROOT
#     output: 
#         cfg.ROOT / "metric_file.tsv"
#     run:
#         import pandas as pd 
#         file_list1 = input.file_clustering
#         file_list2 = [i.removeprefix(input.root+"/") for i in file_list1]
#         df = pd.DataFrame(file_list2)
#         df = df.rename(columns={0: "file"})
#         df[['scenario', 'workflow','method','feature_type','tile_size','distance','ndim','filename']] = df['file'].str.split("/",  expand=True)
#         df["resolution"] = df["filename"].str.removeprefix("r").str.removesuffix("_metrics.tsv")
#         df.to_csv(output[0], sep='\t', header=True, index=False)

rule evaluation:
    input:
        # rules.evaluated.input.file,
        rules.get_metric_file_table.output
    message: "Evaluation done"